---
title: "CIS6060 Assignment 2: Performance of Logistic Regression and Support Vector Machines Classifiers on Three Datasets"
author: "J Moggridge"
date: "25/03/2021"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r tidy_data}
library(tidyverse)
library(ggbeeswarm)
library(glue)

# combine all data to work with
df <- 
  bind_rows(
    read_csv("./results/kfold_abalone.csv", skip = 1L) %>% 
      mutate(data = "Abalone"),
    read_csv("./results/kfold_cancer.csv") %>% 
      mutate(data = "Cancer"),
    read_csv("./results/kfold_diabetes.csv") %>% 
      mutate(data = "Diabetes")
  ) %>%
  mutate(algorithm = ifelse(algorithm == "Logistic regression", 
                            "LogReg", algorithm)) %>% 
  pivot_longer(precision:f1, 
               names_to = 'metric', 
               values_to = 'value')
```


```{r summary_t.test}
# compute performance metric means for each (algorithm, dataset) pair
summary_df <- df %>% 
  group_by(algorithm, data, metric) %>% 
  summarize(values = list(value),
            mean = mean(value), 
            sd = sd(value))
# view(summary_df)

paired_t_test <- function(x,y) 
  t.test(as.vector(x), as.vector(y), paired=T)

# pivot data such that each row has (data, metric, values_SVC, values_LogReg)
pairs_df <- summary_df %>% 
  mutate(id = row_number()) %>% 
  pivot_wider(id_cols = c(data, metric), 
              names_from = algorithm, 
              values_from = c(values, mean, sd)) %>% 
  # do a paired t-test for each dataset & metric
  mutate(t_test = map2(values_LogReg, values_SVC, paired_t_test)) %>% 
  mutate(p_value = map_dbl(t_test, "p.value"),
         mean_diff = map_dbl(t_test, "estimate")) %>% 
  select(everything(), t_test, -contains("values_"))
```

---


*I chose to add the popular Pima Indians diabetes dataset to the pipeline (https://www.kaggle.com/uciml/pima-indians-diabetes-database). These data were previously obtained and additional features were generated in R using an existing script (make_diabetes_data.R).*


# Results

The performance of logistic regression and support vector machine classifiers were compared for three datasets: abalone (n = 4177, 7 variables, 3 classes); cancer (n = 79, 7129 genes, binary outcome, PCA transform was applied); and the Pima Indians diabetes dataset with interaction and quadratic terms (n = 768, 45 predictors, binary outcome). Performance metrics of interest (accuracy, precision, recall, and f1 score) were collected for hypothesis testing by paired t-tests .

Overall, I found that the support vector machine classifier performed slightly better across datasets and metrics than the logistic regression classifier, however these differences only approached significance at the p<0.05 level (fig 1., table 1). 

Perhaps if 10-fold cross validation were performed, the sample size would allow for lower-error estimates of performance, but the cancer datasets is of very small size, making 10-fold cross-validation unfeasible.



```{r plot1, fig.height=4, fig.width=5, fig.cap="Performance metrics (accuracy, f1, precision, and recall) in 5-fold cross-validation of logistic regression and support vector classifiers for Abalone, Cancer, and Diabetes datasets. Means +/- sd are shown as black point and errorbars, with individual observations are shown as grey circles. Note that different scales are used for each dataset."}

ggplot() +
 geom_quasirandom(
   data=df, 
   aes(x = algorithm, y = value), 
   groupOnX = T,
   width = 0.25, 
   color = 'darkgray',
   shape = 1,
   alpha = 0.9,
   method = 'quasirandom') +
  geom_point(
    data=summary_df, 
    aes(x = algorithm, y = mean),
    color = 'black', 
    alpha = 0.45) +
  geom_errorbar(
    data=summary_df, 
    aes(x = algorithm, ymin = mean-sd, ymax = mean+sd),
    color = 'black', 
    width = 0.25,
    alpha = 0.45
  ) +
  facet_grid(data~metric, scales = 'free') +
  labs(x="Classifier", y = NULL) +
  theme_bw() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    )
  
```






----

```{r table1}
# none are significant
pairs_df %>%
  select(data, metric, contains('LogReg'), contains('SVC'), p_value) %>%
  mutate(across(where(is.numeric), ~ round(.x, 2))) %>% 
  mutate(`Logistic Regression` = glue("{mean_LogReg} ({sd_LogReg})"),
         SVM = glue("{mean_SVC} ({sd_SVC})")) %>% 
  select(-contains("mean"), -contains("sd")) %>% 
  rename(Dataset = data, Metric = metric) %>% 
  relocate(p_value,.after = everything()) %>% 
  pander::pander(caption = "Estimates of performance from 5-fold cross-validation of logistic regression and SVM classifiers applied to 3 datasets. Data are expressed as 'means (standard deviation)'")
```